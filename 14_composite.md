# composite workflows

https://github.com/SwissDataScienceCenter/renku-python/issues/706 this is an interesting issue (marked as "needs design" as of Apr 2023).

the answer by panaetius is quite illuminating: 

```
As discussed in the "2020-02-20 Renku Design meeting - Datasets" it'd be nice to have two axis of datasets, explicit vs implicit, and permanent vs ephemeral. Writing this down so it doesn't get lost.

explicit = user created, with user added metadata
implicit = generated by renku, not necessarily visible to the user, an internal abstraction.

And all renku run commands will act on datasets (maybe call it data collections and only use "Dataset" for explicit, user created ones), simplifying implementation on our end (no separate code paths for files or datasets.

The ephemeral/permanent dimension ties into renku run outputs being treated as a dataset, with ephemeral ones just containing the metadata (and the workflow having to be re-run if someone else wants the data), and permanent ones consisting of metadata+data/files. This way, we don't need to bundle code with datasets (as some other data repositories do it) but we can instead use input dataset + renku run workflow --> output dataset and treat the output dataset as something that can be imported into other renku projects that will re-execute the code on that end when needed. And ephemeral would help deal with cases where the data of a dataset can't be published but the metadata should still be in the KG.
```

## omnibenchmark context

I think this highlights the problem: we're tied to the "bundle code with data"
scenario. we need to be able to declare a higher-level perspective on datasets
and derived datasets.

I think this is right what we'd need: the ability to point to a reference
dataset (filtering steps generate secondary datasets, but you can validate that
they're properly derived and updated from the upstream dataset source), and
detach that reference dataset from the internal representation (the ZODB blob).
in that way you probably can stop manually importing the data blob into each
method's repo

I have an idea of how to achieve this, but it's probably good to speak to
panaetius to make sure the design would be aligned with whatever they were
thinking. I can also work on a PoC and talk to them afterwards

## renku run 

- I note that every `renku run` execution generates an internal representation.
  I think this is a lot of noise. same dataset commit + same code commit =>
should not produce a different output (it's just polluting the knowledge
graph). is this feasible though?

- I find slightly counter-intuitive that every renku run generates an internal
  representation in the database, no matter if the
origin dataset has not changed, and the script/workflow has not changed either.
I guess it's just internal pollution one has to live with, but at some point I
guess it'd be nice to address that (as I see it, if we already have a upstream
dataset version ID + method commit ID tuple, there'd be no need to re-run the
workflow, or at least no need to generate redundant triples... the former we
have little control about, but the later is more actionable from our side...



