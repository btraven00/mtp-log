# composite workflows

https://github.com/SwissDataScienceCenter/renku-python/issues/706 this is an interesting issue (marked as "needs design" as of Apr 2023).

the answer by panaetius is quite interesting: 

As discussed in the "2020-02-20 Renku Design meeting - Datasets" it'd be nice to have two axis of datasets, explicit vs implicit, and permanent vs ephemeral. Writing this down so it doesn't get lost.

explicit = user created, with user added metadata
implicit = generated by renku, not necessarily visible to the user, an internal abstraction.

And all renku run commands will act on datasets (maybe call it data collections and only use "Dataset" for explicit, user created ones), simplifying implementation on our end (no separate code paths for files or datasets.

The ephemeral/permanent dimension ties into renku run outputs being treated as a dataset, with ephemeral ones just containing the metadata (and the workflow having to be re-run if someone else wants the data), and permanent ones consisting of metadata+data/files. This way, we don't need to bundle code with datasets (as some other data repositories do it) but we can instead use input dataset + renku run workflow --> output dataset and treat the output dataset as something that can be imported into other renku projects that will re-execute the code on that end when needed. And ephemeral would help deal with cases where the data of a dataset can't be published but the metadata should still be in the KG.

## omnibenchmark context

I think this highlights the problem: we're tied to the "bundle code with data" scenario. we need to be able to declare a higher-level perspective on datasets and derived datasets.
